# portfolio
# Data Scientist

#### Technical Skills: Python, SQL, AWS, Snowflake, MATLAB
## Montreal AI Symposium 2024 Contribution

**Project Accepted and Presented**: Presented my research poster on advanced radar-based human detection and fall detection at the Montreal AI Symposium, showcasing my work on integrating radar technology for emergency response systems in eldercare.
![mais2024](/assets/img/Image.jpg)
## Work & Research Experience
**Computer vision Researcher in Radar Technology @ LITIV Polytechnique Montreal, Morphee+ Collaborative (_Mai 2022 - Decembre 2024_)**
<br />
Advanced Human Detection and Noise Reduction for Autonomous Systems
<br />
- Developed a first-of-its-kind real-time emergency response solution for eldercare, using raw radar data without preprocessing, resulting in a 40% faster emergency detection time compared to conventional systems.

- Designed and implemented deep learning architectures combining Transformers and CNNs to directly analyze complex radar signals, achieving 95% accuracy in detecting human positions (angle and distance).

- Utilized C++ for radar data acquisition, processing over 10,000 hours of radar data for precise identification of human activity and fall detection.

- Developed noise detection and elimination techniques, improving signal clarity by 35% and enhancing activity detection reliability.

- Integrated image data with radar-based detection, leading to a 30% improvement in identifying complex activities.

- Demonstrated expertise in sensor fusion, crucial for autonomous driving applications like object detection, tracking, and anomaly detection.

_Key Technologies & Skills: Radar Signal Processing, Noise Detection, Transformers, CNNs, C++, Python, Complex Data Analysis, Autonomous Driving, Sensor Fusion_

**Vision-Language Researcher for Autonomous Navigation @ Mila  _(Janaury 2024 -April 2024)_**
<br />
Vision-Language Empowered Scene Understanding for Real-Time Autonomous Navigation

Developed an innovative Vision-Language Model (VLM) to enhance object detection and generate descriptive, context-aware captions for complex driving scenes using the KITTI dataset.

Fine-tuned CLIP to identify key road elements (vehicles, pedestrians, signs) and generate informative captions, achieving 85% accuracy in object detection, adding an intelligent language-based layer to scene comprehension.

Integrated multimodal sensor fusion combining camera and LiDAR data, improving recognition precision by 20% compared to vision-only models, making scene understanding more robust and spatially informed.

Optimized real-time inference speed to under 150 milliseconds per frame, ensuring suitability for near-real-time autonomous driving applications.

Deployed the solution using Docker for easy replication, scalability, and accessibility for ongoing research and further model development.

_Key Technologies & Skills: Vision-Language Models (CLIP), PyTorch, Hugging Face Transformers, KITTI Dataset, Multimodal Sensor Fusion, Autonomous Driving, Python, Docker, Real-Time Systems_

**Computer Vision & Deep Learning Researcher @ Mila** _(Septembre 2023 -December 2023)_
<br />
Transformer-Based Image Segmentation for Urban Scene Analysis

Developed a hybrid Vision Transformer and CNN model for semantic segmentation of urban scenes using the Cityscapes dataset, achieving an IoU of 80% for key classes.

Fine-tuned a Vision Transformer (ViT) to label each pixel in high-resolution images, leading to a 15% improvement in accuracy compared to ViT-only models.

Applied data augmentation and mixed-precision training techniques, reducing false negatives by 20% for vulnerable road users.

Deployed the model using Docker, achieving real-time image segmentation at 10 FPS, suitable for urban monitoring and autonomous navigation applications.

Key Technologies & Skills: Vision Transformers (ViT), CNNs, Cityscapes Dataset, PyTorch, Data Augmentation, Semantic Segmentation, Docker, Mixed-Precision Training, Real-Time Systems
<br />
**Research Intern in NLP for Kubernetes @ Université Laval (_July 2022 - November 2022_)**
<br />
Chatbot Development Using Large Language Models for Kubernetes Queries

Developed a chatbot for 70+ students, enabling 80% of them to independently resolve common Kubernetes issues, reducing the need for one-on-one support by 60%.

Fine-tuned GPT-2 and BERT using Hugging Face Transformers, leading to a 30% increase in response accuracy and an average user satisfaction score of 4.6/5.

Implemented transfer learning techniques to adapt language models to Kubernetes-specific terminology and use cases.

Optimized model performance through quantization and knowledge distillation, reducing latency and improving accessibility in low-resource environments.

Enhanced expert-level interactions with specialized prompts to handle 95% of advanced Kubernetes concepts.

Leveraged Kubernetes for model deployment, facilitating efficient container management and system scaling.

Key Technologies & Skills: Hugging Face Transformers (GPT-2, BERT), Large Language Models (LLMs), Kubernetes, Transfer Learning, Model Optimization, Python, NLP, Cloud Deployment

## Education
- Ph.D., Physics | The University of Texas at Dallas (_May 2022_)								       		
- M.S., Physics	| The University of Texas at Dallas (_December 2019_)	 			        		
- B.S., Physics | The University of Texas at Dallas (_May 2017_)

## Work Experience
**Data Scientist @ Toyota Financial Services (_June 2022 - Present_)**
- Uncovered and corrected missing step in production data pipeline which impacted over 70% of active accounts
- Redeveloped loan originations model which resulted in 50% improvement in model performance and saving 1 million dollars in potential losses

**Data Science Consultant @ Shawhin Talebi Ventures LLC (_December 2020 - Present_)**
- Conducted data collection, processing, and analysis for novel study evaluating the impact of over 300 biometrics variables on human performance in hyper-realistic, live-fire training scenarios
- Applied unsupervised deep learning approaches to longitudinal ICU data to discover novel sepsis sub-phenotypes

## Projects
### Data-Driven EEG Band Discovery with Decision Trees
[Publication](https://www.mdpi.com/1424-8220/22/8/3048)

Developed objective strategy for discovering optimal EEG bands based on signal power spectra using **Python**. This data-driven approach led to better characterization of the underlying power spectrum by identifying bands that outperformed the more commonly used band boundaries by a factor of two. The proposed method provides a fully automated and flexible approach to capturing key signal components and possibly discovering new indices of brain activity.

![EEG Band Discovery](/assets/img/eeg_band_discovery.jpeg)

### Decoding Physical and Cognitive Impacts of Particulate Matter Concentrations at Ultra-Fine Scales
[Publication](https://www.mdpi.com/1424-8220/22/11/4240)

Used **Matlab** to train over 100 machine learning models which estimated particulate matter concentrations based on a suite of over 300 biometric variables. We found biometric variables can be used to accurately estimate particulate matter concentrations at ultra-fine spatial scales with high fidelity (r2 = 0.91) and that smaller particles are better estimated than larger ones. Inferring environmental conditions solely from biometric measurements allows us to disentangle key interactions between the environment and the body.

![Bike Study](/assets/img/bike_study.jpeg)

## Talks & Lectures
- Causality: The new science of an old question - GSP Seminar, Fall 2021
- Guest Lecture: Dimensionality Reduction - Big Data and Machine Learning for Scientific Discovery (PHYS 5336), Spring 2021
- Guest Lecture: Fourier and Wavelet Transforms - Scientific Computing (PHYS 5315), Fall 2020
- A Brief Introduction to Optimization - GSP Seminar, Fall 2019
- Weeks of Welcome Poster Competition - UTD, Fall 2019
- A Brief Introduction to Networks - GSP Seminar, Spring 2019

- [Data Science YouTube](https://www.youtube.com/channel/UCa9gErQ9AE5jT2DZLjXBIdA)

## Publications
1. Talebi S., Lary D.J., Wijeratne L. OH., and Lary, T. Modeling Autonomic Pupillary Responses from External Stimuli Using Machine Learning (2019). DOI: 10.26717/BJSTR.2019.20.003446
2. Wijeratne, L.O.; Kiv, D.R.; Aker, A.R.; Talebi, S.; Lary, D.J. Using Machine Learning for the Calibration of Airborne Particulate Sensors. Sensors 2020, 20, 99.
3. Lary, D.J.; Schaefer, D.; Waczak, J.; Aker, A.; Barbosa, A.; Wijeratne, L.O.H.; Talebi, S.; Fernando, B.; Sadler, J.; Lary, T.; Lary, M.D. Autonomous Learning of New Environments with a Robotic Team Employing Hyper-Spectral Remote Sensing, Comprehensive In-Situ Sensing and Machine Learning. Sensors 2021, 21, 2240. https://doi.org/10.3390/s21062240
4. Zhang, Y.; Wijeratne, L.O.H.; Talebi, S.; Lary, D.J. Machine Learning for Light Sensor Calibration. Sensors 2021, 21, 6259. https://doi.org/10.3390/s21186259
5. Talebi, S.; Waczak, J.; Fernando, B.; Sridhar, A.; Lary, D.J. Data-Driven EEG Band Discovery with Decision Trees. Preprints 2022, 2022030145 (doi: 10.20944/preprints202203.0145.v1).
6. Fernando, B.A.; Sridhar, A.; Talebi, S.; Waczak, J.; Lary, D.J. Unsupervised Blink Detection Using Eye Aspect Ratio Values. Preprints 2022, 2022030200 (doi: 10.20944/preprints202203.0200.v1).
7. Talebi, S. et al. Decoding Physical and Cognitive Impacts of PM Concentrations at Ultra-fine Scales, 29 March 2022, PREPRINT (Version 1) available at Research Square [https://doi.org/10.21203/rs.3.rs-1499191/v1]
8. Lary, D.J. et al. (2022). Machine Learning, Big Data, and Spatial Tools: A Combination to Reveal Complex Facts That Impact Environmental Health. In: Faruque, F.S. (eds) Geospatial Technology for Human Well-Being and Health. Springer, Cham. https://doi.org/10.1007/978-3-030-71377-5_12
9. Wijerante, L.O.H. et al. (2022). Advancement in Airborne Particulate Estimation Using Machine Learning. In: Faruque, F.S. (eds) Geospatial Technology for Human Well-Being and Health. Springer, Cham. https://doi.org/10.1007/978-3-030-71377-5_13

- [Data Science Blog](https://medium.com/@shawhin)
